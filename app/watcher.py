import os
import logging
import time
import random
from pymongo import MongoClient
from config import DB_NAME, COLLECTION_NAME

logger = logging.getLogger(__name__)

def run_watch(batch_size=3, max_time_minutes=8):
    """Met √† jour les CVs par petits lots pour √©viter les timeouts"""
    start_time = time.time()
    max_time_seconds = max_time_minutes * 60
    
    try:
        logger.info(f"üîç D√©but mise √† jour CVs (lots de {batch_size}, max {max_time_minutes}min)")
        
        # Import des modules n√©cessaires
        from app.utils.drive_utils import connect_to_drive, list_pdfs, download_file
        from app.utils.enrich_db import process_and_insert_cv, get_mongo_collection
        from app.utils.vectorize import update_faiss_index, sync_faiss_with_db
        
        # Connexion MongoDB pour tracker les CVs vus
        from config import get_mongo_client
        client = get_mongo_client()
        
        if not client:
            logger.error("‚ùå Impossible de se connecter √† MongoDB")
            return False
            
        db = client[DB_NAME]
        collection = db[COLLECTION_NAME]
        seen_collection = db["seen_cvs"]

        def get_seen():
            """R√©cup√®re les IDs d√©j√† trait√©s depuis MongoDB"""
            try:
                seen_doc = seen_collection.find_one({"_id": "seen_files"})
                return set(seen_doc.get("file_ids", [])) if seen_doc else set()
            except Exception as e:
                logger.error(f"Erreur r√©cup√©ration seen files: {e}")
                return set()

        def save_seen(seen_ids):
            """Sauvegarde les IDs trait√©s dans MongoDB"""
            try:
                seen_collection.replace_one(
                    {"_id": "seen_files"}, 
                    {"_id": "seen_files", "file_ids": list(seen_ids)}, 
                    upsert=True
                )
                logger.info(f"üìù Sauvegarde: {len(seen_ids)} fichiers track√©s")
            except Exception as e:
                logger.error(f"Erreur sauvegarde seen files: {e}")

        # V√©rifier la collection MongoDB
        if collection is None:
            logger.error("‚ùå Collection MongoDB non disponible")
            return False
        # Optimisation : r√©utiliser la collection MongoDB pour tous les traitements
        
        initial_count = collection.count_documents({})
        logger.info(f"üìä CVs en base au d√©part: {initial_count}")
        
        # Connexion Google Drive
        try:
            service = connect_to_drive()
            logger.info("‚úÖ Connexion Google Drive r√©ussie")
        except Exception as e:
            logger.error(f"‚ùå Erreur connexion Google Drive: {e}")
            return False
        
        # Listage des PDFs
        from config import GOOGLE_DRIVE_FOLDER_ID
        folder_id = GOOGLE_DRIVE_FOLDER_ID
        
        try:
            pdfs = list_pdfs(service, folder_id)
            logger.info(f"üìÅ {len(pdfs)} fichiers trouv√©s dans Google Drive")
        except Exception as e:
            logger.error(f"‚ùå Erreur listage PDFs: {e}")
            return False

        if not pdfs:
            logger.warning("‚ö†Ô∏è Aucun PDF trouv√© dans le dossier")
            return True

        # R√©cup√©ration des fichiers d√©j√† vus
        seen = get_seen()
        new_seen = set(seen)
        processed_count = 0
        error_count = 0

        # Filtrer les PDFs non trait√©s
        pdfs_to_process = [pdf for pdf in pdfs if pdf["id"] not in seen]
        # Randomiser l'ordre de traitement pour √©viter de retomber toujours sur les m√™mes PDFs probl√©matiques
        random.shuffle(pdfs_to_process)
        logger.info(f"üîÑ {len(pdfs_to_process)} PDFs √† traiter sur {len(pdfs)} total (ordre al√©atoire)")

        if not pdfs_to_process:
            logger.info("‚úÖ Tous les PDFs sont d√©j√† trait√©s")
            return True

        # Traitement par lots
        for batch_start in range(0, len(pdfs_to_process), batch_size):
            # V√©rifier le temps √©coul√©
            elapsed_time = time.time() - start_time
            if elapsed_time > max_time_seconds:
                logger.warning(f"‚è∞ Timeout atteint ({max_time_minutes}min), arr√™t du traitement")
                break
            
            batch_end = min(batch_start + batch_size, len(pdfs_to_process))
            batch = pdfs_to_process[batch_start:batch_end]
            
            logger.info(f"üì¶ Lot {batch_start//batch_size + 1}: traitement de {len(batch)} PDFs")
            
            for i, pdf in enumerate(batch):
                pdf_id = pdf["id"]
                filename = pdf["name"]
                
                # V√©rifier le temps pour chaque fichier
                elapsed_time = time.time() - start_time
                if elapsed_time > max_time_seconds:
                    logger.warning(f"‚è∞ Timeout pendant traitement de {filename}")
                    break
                
                logger.info(f"üìÑ [{batch_start + i + 1}/{len(pdfs_to_process)}] {filename}")
                
                try:
                    # T√©l√©chargement avec timeout court
                    logger.info(f"‚¨áÔ∏è T√©l√©chargement...")
                    download_success = download_file(service, pdf_id, filename)
                    
                    if not download_success:
                        logger.warning(f"‚ö†Ô∏è √âchec t√©l√©chargement {filename}")
                        error_count += 1
                        continue
                    
                    # Traitement avec monitoring du temps
                    processing_start = time.time()
                    logger.info(f"üîÑ Traitement...")
                    success = process_and_insert_cv(filename, collection=collection)
                    processing_time = time.time() - processing_start
                    
                    if success:
                        logger.info(f"‚úÖ {filename} trait√© en {processing_time:.1f}s")
                        new_seen.add(pdf_id)  # Ajout √† seen uniquement si insertion r√©ussie
                        processed_count += 1
                    else:
                        logger.warning(f"‚ö†Ô∏è √âchec traitement {filename} (non ajout√© √† seen_cvs)")
                        error_count += 1
                    
                    # Nettoyage imm√©diat
                    try:
                        if os.path.exists(filename):
                            os.remove(filename)
                    except Exception as cleanup_error:
                        logger.warning(f"‚ö†Ô∏è Nettoyage {filename}: {cleanup_error}")
                    
                    # Petite pause entre les fichiers pour √©viter la surcharge
                    time.sleep(1)
                        
                except Exception as e:
                    logger.error(f"‚ùå Erreur traitement {filename}: {e}")
                    error_count += 1
                    
                    # Nettoyer en cas d'erreur
                    try:
                        if os.path.exists(filename):
                            os.remove(filename)
                    except:
                        pass
            
            # Sauvegarder apr√®s chaque lot
            save_seen(new_seen)
            logger.info(f"üíæ Progression sauvegard√©e: {processed_count} trait√©s")
            
            # Pause entre les lots
            if batch_end < len(pdfs_to_process):
                logger.info("‚è∏Ô∏è Pause 3s entre les lots...")
                time.sleep(3)

        # Statistiques finales
        final_count = collection.count_documents({})
        added_count = final_count - initial_count
        total_time = time.time() - start_time
        
        logger.info(f"üìä Statistiques finales:")
        logger.info(f"   - Temps total: {total_time:.1f}s")
        logger.info(f"   - PDFs trait√©s: {processed_count}")
        logger.info(f"   - Erreurs: {error_count}")
        logger.info(f"   - CVs en base: {initial_count} ‚Üí {final_count} (+{added_count})")
        
        # Mettre √† jour l'index FAISS si de nouveaux CVs ont √©t√© ajout√©s
        # if processed_count > 0:
        #     logger.info("üîÑ Mise √† jour index FAISS...")
        #     try:
        #         faiss_success = update_faiss_index()
        #         if faiss_success:
        #             logger.info("‚úÖ Index FAISS mis √† jour")
        #         else:
        #             logger.warning("‚ö†Ô∏è Erreur FAISS")
        #     except Exception as e:
        #         logger.error(f"‚ùå Erreur FAISS: {e}")
        # L'ajout incr√©mental FAISS est d√©j√† fait dans enrich_db, inutile de revectoriser toute la base ici.
        
        # Indiquer s'il reste des fichiers √† traiter
        remaining = len(pdfs_to_process) - processed_count - error_count
        if remaining > 0:
            logger.info(f"‚ÑπÔ∏è {remaining} PDFs restants √† traiter (relancez la mise √† jour)")
        else:
            logger.info("‚úÖ Tous les PDFs ont √©t√© trait√©s")
        
        logger.info("‚úÖ Mise √† jour termin√©e")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur globale dans run_watch: {e}")
        return False

if __name__ == "__main__":
    # Configuration du logging pour les tests
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("üöÄ Test de mise √† jour des CVs par lots")
    success = run_watch(batch_size=2, max_time_minutes=5)
    
    if success:
        print("‚úÖ Test r√©ussi")
    else:
        print("‚ùå Test √©chou√©")
