import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from bson import ObjectId
import logging
import base64
import tempfile
import os
from config import DB_NAME, COLLECTION_NAME, HF_TOKEN

if not os.environ.get("HF_TOKEN"):
    import logging
    logging.warning("‚ö†Ô∏è La variable d'environnement HF_TOKEN n'est pas d√©finie. Les appels √† Hugging Face risquent d'√©chouer (erreur 429). Ajoutez HF_TOKEN dans la configuration de votre h√©bergeur !")

model = SentenceTransformer("paraphrase-MiniLM-L3-v2", use_auth_token=HF_TOKEN)

logger = logging.getLogger(__name__)

def update_faiss_index(client=None):
    """Met √† jour l'index FAISS et le stocke dans MongoDB"""
    try:
        logger.info("üîÑ D√©but cr√©ation index FAISS")
        
        # Utiliser le mod√®le global d√©j√† charg√©
        # model = SentenceTransformer("all-MiniLM-L6-v2")  # SUPPRIM√â
        
        if client is None:
            from config import get_mongo_client
            client = get_mongo_client()
            
        if not client:
            logger.error("‚ùå Impossible de se connecter √† MongoDB")
            return False
            
        db = client[DB_NAME]
        collection = db[COLLECTION_NAME]
        index_collection = db["faiss_index"]
        
        # cvs = list(collection.find({}))
        # logger.info(f"üìä Traitement de {len(cvs)} CVs pour indexation")
        logger.info("üìä D√©but du traitement des CVs pour indexation (streaming)")
        vectors = []
        id_mapping = []
        processed_count = 0

        for cv in collection.find({}):
            try:
                # Construire le texte √† vectoriser
                text_parts = []
                
                # Nom
                nom = cv.get("nom", "")
                if nom and isinstance(nom, str):
                    text_parts.append(nom)
                
                # Comp√©tences
                competences = cv.get("competences", [])
                if competences and isinstance(competences, list):
                    comp_text = " ".join([str(c) for c in competences if c])
                    if comp_text.strip():
                        text_parts.append(comp_text)
                
                # Biographie
                biographie = cv.get("biographie", "")
                if biographie and isinstance(biographie, str) and biographie.strip():
                    text_parts.append(biographie)
                
                # Secteur
                secteur = cv.get("secteur", [])
                if secteur:
                    if isinstance(secteur, list):
                        secteur_text = " ".join([str(s) for s in secteur if s])
                    else:
                        secteur_text = str(secteur)
                    
                    if secteur_text.strip():
                        text_parts.append(secteur_text)
                
                # Exp√©riences (titre + entreprise + description)
                experiences = cv.get("experiences", [])
                if experiences and isinstance(experiences, list):
                    for exp in experiences[:3]:  # Limiter aux 3 premi√®res exp√©riences
                        if isinstance(exp, dict):
                            exp_parts = []
                            
                            titre = exp.get("titre", "")
                            if titre:
                                exp_parts.append(str(titre))
                            
                            entreprise = exp.get("entreprise", "")
                            if entreprise:
                                exp_parts.append(str(entreprise))
                            
                            description = exp.get("description", "")
                            if description and len(str(description)) > 10:  # Ignorer les descriptions trop courtes
                                exp_parts.append(str(description)[:500])  # Limiter √† 500 caract√®res
                            
                            if exp_parts:
                                text_parts.append(" ".join(exp_parts))
                
                # Formations
                formations = cv.get("formations", [])
                if formations and isinstance(formations, list):
                    for form in formations[:2]:  # Limiter aux 2 premi√®res formations
                        if isinstance(form, dict):
                            form_parts = []
                            
                            diplome = form.get("diplome", "")
                            if diplome:
                                form_parts.append(str(diplome))
                            
                            etablissement = form.get("etablissement", "")
                            if etablissement:
                                form_parts.append(str(etablissement))
                            
                            if form_parts:
                                text_parts.append(" ".join(form_parts))
                
                # Cr√©er le texte final
                if text_parts:
                    combined_text = " ".join(text_parts)
                    
                    # Nettoyer le texte
                    combined_text = " ".join(combined_text.split())  # Supprimer espaces multiples
                    combined_text = combined_text[:2000]  # Limiter la taille
                    
                    if len(combined_text) > 10:  # Minimum 10 caract√®res
                        # Encoder
                        vector = model.encode(combined_text)
                        
                        # V√©rifier que le vecteur est valide
                        if vector is not None and len(vector) > 0:
                            # Normaliser
                            norm = np.linalg.norm(vector)
                            if norm > 0:
                                vector = vector / norm
                                
                                vectors.append(vector.astype(np.float32))
                                id_mapping.append(str(cv["_id"]))
                                processed_count += 1
                                
                                if processed_count % 10 == 0:
                                    logger.info(f"üìä {processed_count} CVs vectoris√©s...")
                            else:
                                logger.warning(f"‚ö†Ô∏è Vecteur nul pour {cv.get('nom', 'CV sans nom')}")
                        else:
                            logger.warning(f"‚ö†Ô∏è √âchec encodage pour {cv.get('nom', 'CV sans nom')}")
                    else:
                        logger.warning(f"‚ö†Ô∏è Texte trop court pour {cv.get('nom', 'CV sans nom')}")
                else:
                    logger.warning(f"‚ö†Ô∏è Aucun texte extractible pour {cv.get('nom', 'CV sans nom')}")
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur vectorisation CV {cv.get('nom', 'inconnu')}: {e}")
                continue

        if not vectors:
            logger.error("‚ùå Aucun vecteur cr√©√©")
            return False

        logger.info(f"‚úÖ {len(vectors)} vecteurs cr√©√©s avec succ√®s")

        # Cr√©ation de l'index FAISS
        vectors_array = np.array(vectors, dtype=np.float32)
        dimension = vectors_array.shape[1]
        
        logger.info(f"üìê Dimension des vecteurs: {dimension}")
        
        # Utiliser IndexFlatIP pour vecteurs normalis√©s (produit scalaire)
        index = faiss.IndexFlatIP(dimension)
        index.add(vectors_array)
        
        logger.info(f"üîç Index FAISS cr√©√© avec {index.ntotal} vecteurs")

        # S√©rialisation ROBUSTE avec fichier temporaire
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            temp_path = tmp_file.name
            
        try:
            # √âcrire l'index dans un fichier temporaire
            faiss.write_index(index, temp_path)
            
            # Lire le fichier et encoder en base64
            with open(temp_path, 'rb') as f:
                index_bytes = f.read()
            index_b64 = base64.b64encode(index_bytes).decode('utf-8')
            
        finally:
            # Nettoyer le fichier temporaire
            if os.path.exists(temp_path):
                os.unlink(temp_path)
        
        # S√©rialiser le mapping
        mapping_bytes = pickle.dumps(id_mapping)
        mapping_b64 = base64.b64encode(mapping_bytes).decode('utf-8')

        # Supprimer l'ancien index si il existe
        index_collection.delete_one({"_id": "faiss_data"})
        
        # Stockage dans MongoDB
        doc_to_insert = {
            "_id": "faiss_data",
            "index": index_b64,
            "id_mapping": mapping_b64,
            "vector_count": len(vectors),
            "dimension": dimension,
            "model_name": "all-MiniLM-L6-v2",
            "index_type": "IndexFlatIP"
        }
        
        index_collection.insert_one(doc_to_insert)
        
        logger.info(f"üíæ Index FAISS stock√© en base ({len(vectors)} profils, dimension {dimension})")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation index FAISS: {e}")
        import traceback
        logger.error(f"Stack trace: {traceback.format_exc()}")
        return False

def load_faiss_from_mongodb(client=None):
    """Charge l'index FAISS depuis MongoDB"""
    try:
        if client is None:
            from config import get_mongo_client
            client = get_mongo_client()
        if not client:
            logger.error("‚ùå Impossible de se connecter √† MongoDB pour FAISS")
            return None, []
            
        db = client[DB_NAME]
        index_collection = db["faiss_index"]
        
        faiss_doc = index_collection.find_one({"_id": "faiss_data"})
        if not faiss_doc:
            logger.warning("‚ö†Ô∏è Aucun index FAISS trouv√© en base")
            return None, []
        
        logger.info(f"üìñ Chargement index FAISS ({faiss_doc.get('vector_count', 0)} vecteurs)")
        
        # D√©s√©rialisation ROBUSTE avec fichier temporaire
        index_b64 = faiss_doc.get("index")
        if not index_b64:
            logger.error("‚ùå Donn√©es index manquantes")
            return None, []
        
        # D√©coder et √©crire dans un fichier temporaire
        index_bytes = base64.b64decode(index_b64)
        
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            temp_path = tmp_file.name
            tmp_file.write(index_bytes)
        
        try:
            # Charger l'index depuis le fichier temporaire
            index = faiss.read_index(temp_path)
            
        finally:
            # Nettoyer le fichier temporaire
            if os.path.exists(temp_path):
                os.unlink(temp_path)
        
        # Chargement du mapping
        mapping_b64 = faiss_doc.get("id_mapping")
        if not mapping_b64:
            logger.error("‚ùå Mapping des IDs manquant")
            return None, []
            
        mapping_bytes = base64.b64decode(mapping_b64)
        id_mapping = pickle.loads(mapping_bytes)
        
        logger.info(f"‚úÖ Index FAISS charg√©: {len(id_mapping)} profils, dimension {index.d}")
        return index, id_mapping
        
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement index FAISS: {e}")
        import traceback
        logger.error(f"Stack trace: {traceback.format_exc()}")
        return None, []

def search_similar_cvs(query_text, top_k=5):
    """Recherche des CVs similaires √† la requ√™te"""
    try:
        # Charger l'index
        index, id_mapping = load_faiss_from_mongodb()
        
        if index is None or not id_mapping:
            logger.error("‚ùå Index FAISS non disponible pour la recherche")
            return []
        
        logger.info(f"üîç Recherche pour: '{query_text}' (top {top_k})")
        
        # Utiliser le mod√®le global d√©j√† charg√©
        # model = SentenceTransformer("all-MiniLM-L6-v2")  # SUPPRIM√â
        query_vector = model.encode(query_text)
        
        # Normaliser
        norm = np.linalg.norm(query_vector)
        if norm > 0:
            query_vector = query_vector / norm
        
        query_vector = np.array([query_vector], dtype=np.float32)
        
        # Recherche
        scores, indices = index.search(query_vector, min(top_k, len(id_mapping)))
        
        # Retourner les r√©sultats
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx >= 0 and idx < len(id_mapping):  # V√©rifier la validit√© de l'index
                results.append({
                    "cv_id": id_mapping[idx],
                    "score": float(score),
                    "rank": i + 1
                })
        
        logger.info(f"‚úÖ {len(results)} r√©sultats trouv√©s")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå Erreur recherche vectorielle: {e}")
        import traceback
        logger.error(f"Stack trace: {traceback.format_exc()}")
        return []

def clean_faiss_index(client=None):
    """Nettoie l'index FAISS en base (utilitaire de debug)"""
    try:
        if client is None:
            from config import get_mongo_client
            client = get_mongo_client()
        if not client:
            logger.error("‚ùå Impossible de se connecter √† MongoDB")
            return False
            
        db = client[DB_NAME]
        index_collection = db["faiss_index"]
        
        # Supprimer l'ancien index
        result = index_collection.delete_one({"_id": "faiss_data"})
        
        if result.deleted_count > 0:
            logger.info("‚úÖ Index FAISS nettoy√©")
            return True
        else:
            logger.info("‚ÑπÔ∏è Aucun index √† nettoyer")
            return True
            
    except Exception as e:
        logger.error(f"‚ùå Erreur nettoyage index: {e}")
        return False

def build_cv_text_for_vectorization(cv):
    """Construit le texte √† vectoriser pour un CV"""
    text_parts = []
    
    # Nom
    nom = cv.get("nom", "")
    if nom and isinstance(nom, str):
        text_parts.append(nom)
    
    # Comp√©tences
    competences = cv.get("competences", [])
    if competences and isinstance(competences, list):
        comp_text = " ".join([str(c) for c in competences if c])
        if comp_text.strip():
            text_parts.append(comp_text)
    
    # Biographie
    biographie = cv.get("biographie", "")
    if biographie and isinstance(biographie, str) and biographie.strip():
        text_parts.append(biographie)
    
    # Secteur
    secteur = cv.get("secteur", [])
    if secteur:
        if isinstance(secteur, list):
            secteur_text = " ".join([str(s) for s in secteur if s])
        else:
            secteur_text = str(secteur)
        
        if secteur_text.strip():
            text_parts.append(secteur_text)
    
    # Exp√©riences (titre + entreprise + description)
    experiences = cv.get("experiences", [])
    if experiences and isinstance(experiences, list):
        for exp in experiences[:3]:  # Limiter aux 3 premi√®res exp√©riences
            if isinstance(exp, dict):
                exp_parts = []
                
                titre = exp.get("titre", "")
                if titre:
                    exp_parts.append(str(titre))
                
                entreprise = exp.get("entreprise", "")
                if entreprise:
                    exp_parts.append(str(entreprise))
                
                description = exp.get("description", "")
                if description and len(str(description)) > 10:  # Ignorer les descriptions trop courtes
                    exp_parts.append(str(description)[:500])  # Limiter √† 500 caract√®res
                
                if exp_parts:
                    text_parts.append(" ".join(exp_parts))
    
    # Formations
    formations = cv.get("formations", [])
    if formations and isinstance(formations, list):
        for form in formations[:2]:  # Limiter aux 2 premi√®res formations
            if isinstance(form, dict):
                form_parts = []
                
                diplome = form.get("diplome", "")
                if diplome:
                    form_parts.append(str(diplome))
                
                etablissement = form.get("etablissement", "")
                if etablissement:
                    form_parts.append(str(etablissement))
                
                if form_parts:
                    text_parts.append(" ".join(form_parts))
    
    if not text_parts:
        logger.warning(f"‚ö†Ô∏è Aucun texte extractible pour {cv.get('nom', 'CV sans nom')}")
        return None
    
    combined_text = " ".join(text_parts)
    combined_text = " ".join(combined_text.split())  # Normaliser les espaces
    combined_text = combined_text[:2000]  # Limiter la longueur
    
    if len(combined_text) <= 10:
        logger.warning(f"‚ö†Ô∏è Texte trop court pour {cv.get('nom', 'CV sans nom')}")
        return None
    
    return combined_text

def save_faiss_to_mongodb(index, id_mapping):
    """Sauvegarde l'index FAISS et le mapping dans MongoDB"""
    try:
        from config import get_mongo_client, DB_NAME
        
        client = get_mongo_client()
        if not client:
            logger.error("‚ùå Impossible de se connecter √† MongoDB pour sauvegarde FAISS")
            return False
        
        db = client[DB_NAME]
        index_collection = db["faiss_index"]
        
        # S√©rialisation FAISS
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            temp_path = tmp_file.name
        
        try:
            faiss.write_index(index, temp_path)
            with open(temp_path, 'rb') as f:
                index_bytes = f.read()
            index_b64 = base64.b64encode(index_bytes).decode('utf-8')
        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)
        
        # S√©rialisation mapping
        mapping_bytes = pickle.dumps(id_mapping)
        mapping_b64 = base64.b64encode(mapping_bytes).decode('utf-8')
        
        # Mise √† jour du document en base
        index_collection.update_one(
            {"_id": "faiss_data"},
            {"$set": {
                "index": index_b64,
                "id_mapping": mapping_b64,
                "vector_count": len(id_mapping),
                "dimension": index.d,
                "model_name": "paraphrase-MiniLM-L3-v2",
                "index_type": "IndexFlatIP"
            }},
            upsert=True
        )
        
        logger.info(f"‚úÖ Index FAISS sauvegard√© dans MongoDB ({len(id_mapping)} vecteurs)")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur sauvegarde FAISS dans MongoDB: {e}")
        return False

def add_cv_to_faiss_index(cv):
    """Ajoute un CV √† l'index FAISS de mani√®re incr√©mentale"""
    try:
        # Essayer de charger l'index existant
        existing_index, existing_mapping = load_faiss_from_mongodb()
        
        if existing_index is None:
            # Pas d'index existant, cr√©er un nouvel index
            logger.info("üÜï Cr√©ation d'un nouvel index FAISS")
            vectors = []
            id_mapping = []
            
            # Ajouter le CV actuel
            cv_text = build_cv_text_for_vectorization(cv)
            if cv_text:
                vector = model.encode(cv_text)
                vectors.append(vector)
                id_mapping.append(str(cv["_id"]))
            
            # Cr√©er l'index FAISS
            if vectors:
                vectors_array = np.array(vectors, dtype=np.float32)
                dimension = vectors_array.shape[1]
                new_index = faiss.IndexFlatIP(dimension)
                new_index.add(vectors_array)
                
                # Sauvegarder dans MongoDB
                save_faiss_to_mongodb(new_index, id_mapping)
                logger.info(f"‚úÖ Nouvel index FAISS cr√©√© avec {len(id_mapping)} CV")
                return True
            else:
                logger.warning("‚ö†Ô∏è Impossible de vectoriser le CV")
                return False
        else:
            # Index existant, ajouter le CV
            cv_text = build_cv_text_for_vectorization(cv)
            if not cv_text:
                logger.warning("‚ö†Ô∏è Impossible de vectoriser le CV")
                return False
            
            vector = model.encode(cv_text)
            vector_array = np.array([vector], dtype=np.float32)
            
            # Ajouter √† l'index existant
            existing_index.add(vector_array)
            existing_mapping.append(str(cv["_id"]))
            
            # Sauvegarder l'index mis √† jour
            save_faiss_to_mongodb(existing_index, existing_mapping)
            logger.info(f"‚úÖ CV ajout√© √† l'index FAISS existant (total: {len(existing_mapping)})")
            return True
            
    except Exception as e:
        logger.error(f"‚ùå Erreur ajout CV √† FAISS: {e}")
        return False

def sync_faiss_with_db(client=None):
    """
    V√©rifie la coh√©rence entre la BDD et l'index FAISS.
    Ajoute √† FAISS tout CV pr√©sent en BDD mais absent du mapping.
    """
    from config import DB_NAME, COLLECTION_NAME
    if client is None:
        from config import get_mongo_client
        client = get_mongo_client()
    if not client:
        logger.error("‚ùå Impossible de se connecter √† MongoDB pour la synchronisation FAISS")
        return False
    db = client[DB_NAME]
    collection = db[COLLECTION_NAME]
    cvs = list(collection.find({}))
    index, id_mapping = load_faiss_from_mongodb(client=client)
    id_mapping_set = set(id_mapping)
    added = 0
    for cv in cvs:
        if str(cv['_id']) not in id_mapping_set:
            logger.warning(f"üü° CV manquant dans FAISS: {cv.get('nom', 'inconnu')} ({cv['_id']})")
            add_cv_to_faiss_index(cv)
            added += 1
    logger.info(f"‚úÖ Synchronisation FAISS termin√©e. {added} CVs ajout√©s √† FAISS.")
    return True
